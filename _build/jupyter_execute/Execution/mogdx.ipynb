{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109b9405",
   "metadata": {},
   "source": [
    "# MOGDx Module\n",
    "\n",
    "```\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys  \n",
    "sys.path.insert(0, './MAIN/')\n",
    "from utils import *\n",
    "from GNN_MME import *\n",
    "from train import *\n",
    "import preprocess_functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import networkx as nx\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Finished Library Import \\n\")\n",
    "\n",
    "def main(args): \n",
    "    \n",
    "    # Check if output directory exists, if not create it\n",
    "    if not os.path.exists(args.output) : \n",
    "        os.makedirs(args.output, exist_ok=True)\n",
    "        \n",
    "    # Specify the device to use\n",
    "    device = torch.device('cpu' if args.no_cuda else 'cuda') # Get GPU device name, else use CPU\n",
    "    print(\"Using %s device\" % device)\n",
    "    get_gpu_memory()\n",
    "\n",
    "    if not R_workflow : \n",
    "        # Load data and metadata\n",
    "        datModalities , meta = data_parsing_python(args.input , args.modalities , args.target , args.index_col)\n",
    "\n",
    "        # Load Network\n",
    "        graph_file = args.input + '/' + '_'.join(args.modalities) + '_graph.graphml'\n",
    "        g = nx.read_graphml(graph_file)\n",
    "    else : \n",
    "        # Load data and metadata\n",
    "        datModalities , meta = data_parsing_R(args.input , args.modalities , args.target , args.index_col)\n",
    "\n",
    "        # Load Network\n",
    "        graph_file = args.input + '/' + '_'.join(args.modalities) + '_graph.csv'\n",
    "        g = network_from_csv(graph_file , False)\n",
    "        nx.set_node_attributes(g , meta.astype('category').cat.codes , 'label')\n",
    "\n",
    "    meta = meta.loc[sorted(meta.index)]\n",
    "\n",
    "    # Generate K Fold splits\n",
    "    if args.no_shuffle : \n",
    "        skf = StratifiedKFold(n_splits=args.n_splits , shuffle=False) \n",
    "    else :\n",
    "        skf = StratifiedKFold(n_splits=args.n_splits , shuffle=True) \n",
    "\n",
    "    print(skf)\n",
    "\n",
    "    # Order model inputs and identify subjects in each modality\n",
    "    subjects_list = [list(set(g.nodes) & set(datModalities[mod].index)) for mod in datModalities]\n",
    "    h = [torch.from_numpy(datModalities[mod].loc[subjects_list[i]].to_numpy(dtype=np.float32)).to(device) for i , mod in enumerate(datModalities) ]\n",
    "    MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "    \n",
    "    del datModalities\n",
    "    gc.collect()\n",
    "\n",
    "    # Get the unique labels in the metadata\n",
    "    labels = F.one_hot(torch.Tensor(list(meta.astype('category').cat.codes)).to(torch.int64)).to(device)\n",
    "\n",
    "    output_metrics = []\n",
    "    test_logits = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(meta.index, meta)) :\n",
    "\n",
    "        # Initialize model\n",
    "        model = GCN_MME(MME_input_shapes , args.latent_dim , args.decoder_dim , args.h_feats  , len(node_subjects.unique())).to(device)\n",
    "        print(model)\n",
    "        print(g)\n",
    "\n",
    "         # Split training data into training and validation sets\n",
    "        train_index , val_index = train_test_split(\n",
    "            train_index, train_size=0.8, test_size=None, stratify=meta.iloc[train_index]\n",
    "            )\n",
    "\n",
    "        # Train the model\n",
    "        loss_plot = train(g, h , subjects_list , train_index , val_index , device ,  model , labels , 2000 , 1e-3 , 100)\n",
    "        plt.title(f'Loss for split {i}')\n",
    "        save_path = args.output + '/loss_plots/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        plt.savefig(f'{save_path}loss_split_{i}.png' , dpi = 200)\n",
    "        plt.clf()\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_output_metrics = evaluate(test_index , device , g , h , subjects_list , model , labels )\n",
    "\n",
    "        print(\n",
    "            \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "            i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    "        )\n",
    "        \n",
    "        # Save the test logits and labels for later analysis\n",
    "        test_logits.extend(test_output_metrics[-1][test_index])\n",
    "        test_labels.extend(labels[test_index])\n",
    "        \n",
    "        # Save the output metrics and best performing model\n",
    "        output_metrics.append(test_output_metrics)\n",
    "        if i == 0 : \n",
    "            best_model = model\n",
    "            best_idx = i\n",
    "        elif output_metrics[best_idx][1] < test_output_metrics[1] : \n",
    "            best_model = model\n",
    "            best_idx   = i\n",
    "\n",
    "        get_gpu_memory()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('Clearing gpu memory')\n",
    "        get_gpu_memory()\n",
    "            \n",
    "    # Save the output metrics to a file   \n",
    "    accuracy = []\n",
    "    F1 = []\n",
    "    output_file = args.output + '/' + \"test_metrics.txt\"\n",
    "    with open(output_file , 'w') as f :\n",
    "        i = 0\n",
    "        for metric in output_metrics :\n",
    "            i += 1\n",
    "            f.write(\"Fold %i \\n\" % i)\n",
    "            f.write(f\"acc = %2.3f , avg_prc = %2.3f , avg_recall = %2.3f , avg_f1 = %2.3f\" % \n",
    "                    (metric[1] , metric[3] , metric[4] , metric[2]))\n",
    "            f.write('\\n')\n",
    "            accuracy.append(metric[1])\n",
    "            F1.append(metric[2])\n",
    "            \n",
    "        f.write('-------------------------\\n')\n",
    "        f.write(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f \\n\" %(args.n_splits , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "        f.write(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f \\n\" %(args.n_splits , np.mean(F1)*100 , np.std(F1)*100))\n",
    "        f.write('-------------------------\\n')\n",
    "\n",
    "    print(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "    print(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(F1)*100 , np.std(F1)*100))\n",
    "    \n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    # Extract month and day as string names\n",
    "    month = current_date.strftime('%B')[:3]  # Full month name\n",
    "    day = current_date.day\n",
    "    \n",
    "    save_path = args.output + '/Models/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model.state_dict(),\n",
    "        # You can add more information to save, such as training history, hyperparameters, etc.\n",
    "    }, f'{save_path}GCN_MME_model_{month}{day}' )\n",
    "    \n",
    "    if args.no_output_plots : \n",
    "        cmplt = confusion_matrix(test_logits , test_labels , meta.astype('category').cat.categories)\n",
    "        plt.title('Test Accuracy = %2.1f %%' % (np.mean(accuracy)*100))\n",
    "        output_file = args.output + '/' + \"confusion_matrix.png\"\n",
    "        plt.savefig(output_file , dpi = 300)\n",
    "        \n",
    "        precision_recall_plot , all_predictions_conf = AUROC(test_logits, test_labels , meta)\n",
    "        output_file = args.output + '/' + \"precision_recall.png\"\n",
    "        precision_recall_plot.savefig(output_file , dpi = 300)\n",
    "\n",
    "        node_predictions = []\n",
    "        display_label = meta.astype('category').cat.categories\n",
    "        for pred in all_predictions_conf.argmax(1)  : \n",
    "            node_predictions.append(display_label[pred])\n",
    "\n",
    "        pd.DataFrame({'Actual' : meta.loc[list(nx.get_node_attributes(g, 'idx').keys())] , 'Predicted' : node_predictions}).to_csv(args.output + '/Predictions.csv')\n",
    "\n",
    "        \n",
    "def construct_parser():\n",
    "    \"\"\"\n",
    "    Construct the argument parser for MOGDx.\n",
    "\n",
    "    Returns:\n",
    "        argparse.ArgumentParser: The argument parser object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='MOGDx')\n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--patience', type=float, default=100,\n",
    "                        help='Early Stopping Patience (default: 100 batches of 5 -> equivalent of 100*5 = 500)')\n",
    "    #parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "    #                    help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    #parser.add_argument('--seed', type=int, default=None, metavar='S',\n",
    "    #                    help='random seed (default: random number)')\n",
    "    #parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "    #                    help='how many batches to wait before logging '\n",
    "    #                    'training status')\n",
    "    parser.add_argument('--no-output-plots', action='store_false' , default=True,\n",
    "                        help='Disables Confusion Matrix and TSNE plots')\n",
    "    parser.add_argument('--split-val', action='store_false' , default=True,\n",
    "                        help='Disable validation split on AE and GNN')\n",
    "    parser.add_argument('--no-shuffle', action='store_true' , default=False,\n",
    "                        help='Disable shuffling of index for K fold split')\n",
    "    parser.add_argument('--psn-only', action='store_true' , default=False,\n",
    "                        help='Dont train on any node features')\n",
    "    parser.add_argument('--no-psn', action='store_true' , default=False,\n",
    "                        help='Dont train on PSN (removal of edges)')\n",
    "    parser.add_argument('--val-split-size', default=0.85 , type=float , help='Validation split of training set in'\n",
    "                        'each k fold split. Default of 0.85 is 60/10/30 train/val/test with a 10 fold split')\n",
    "    parser.add_argument('--index-col' , type=str , default='', \n",
    "                        help ='Name of column in input data which refers to index.'\n",
    "                        'Leave blank if none.')\n",
    "    parser.add_argument('--n-splits' , default=10 , type=int, help='Number of K-Fold'\n",
    "                        'splits to use')\n",
    "    parser.add_argument('--h-feats' , default=64 , type=int , help ='Integer specifying hidden dim of GNN'\n",
    "                        'specifying GNN layer size')\n",
    "    parser.add_argument('--decoder-dim' , default=64 , type=int , help ='Integer specifying dim of common '\n",
    "                        'layer to all modalities')\n",
    "    #parser.add_argument('--layers' , default=[64 , 64], nargs=\"+\" , type=int , help ='List of integrs'\n",
    "    #                    'specifying GNN layer sizes')\n",
    "    #parser.add_argument('--layer-activation', default=['elu' , 'elu'] , nargs=\"+\" , type=str , help='List of activation'\n",
    "    #                    'functions for each GNN layer')\n",
    "    parser.add_argument('--R', action='store_true' , default=False,\n",
    "                    help='Execute from the R workflow')\n",
    "\n",
    "    parser.add_argument('-i', '--input', required=True, help='Path to the '\n",
    "                        'input data for the model to read')\n",
    "    parser.add_argument('-o', '--output', required=True, help='Path to the '\n",
    "                        'directory to write output to')\n",
    "    parser.add_argument('-mod', '--modalities', required=True, help='Name of the'\n",
    "                        'modalities to include in the integration. Must be a list of strings')\n",
    "    parser.add_argument('-ld' , '--latent-dim', required=True, nargs=\"+\", type=int , help='List of integers '\n",
    "                        'corresponding to the length of hidden dims of each data modality')\n",
    "    parser.add_argument('--target' , required = True , help='Column name referring to the'\n",
    "                        'disease classification label')\n",
    "    return parser\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    parser = construct_parser()\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}