

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GNN_MME &mdash; Multi Omic Graph Diagnosis (MOGDx)</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
      <script src="../_static/scripts/sphinx-book-theme.js"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
      <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../intro.html">
            
              <img src="../_static/mogdx.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted.html">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Processing_in_R.html">Processing in R</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Processing_in_Python.html">Processing in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MOGDx Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Executing_MOGDx.html">Executing MOGDx</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MOGDx Model Types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../MOGDx/Notebooks/MOGDx.html">MOGDx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MOGDx/Notebooks/MOGDx_PNet.html">MOGDx &amp; PNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MOGDx/Notebooks/MOGDx_NetGen.html">MOGDx Network Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MOGDx/Notebooks/MOGDx_Free.html">MOGDx Free</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Training_MOGDx.html">MOGDx Main Functions and Classes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contact.html">Contact Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../requirements.html">Requirements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../intro.html">Project name not set</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../intro.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Module code</a></li>
      <li class="breadcrumb-item active">GNN_MME</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for GNN_MME</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DGLBACKEND&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;pytorch&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dgl</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dgl.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">GraphConv</span> <span class="p">,</span> <span class="n">SAGEConv</span><span class="p">,</span> <span class="n">GATConv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dgl.dataloading</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">MultiLayerFullNeighborSampler</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="n">orig_sys_path</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">[:]</span>
<span class="n">dirname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">))</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span> <span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span> <span class="p">,</span> <span class="s1">&#39;../Modules/PNetTorch/MAIN&#39;</span><span class="p">))</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">Pnet</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaskedLinear</span><span class="p">,</span> <span class="n">PNET</span> 
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span> <span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span> <span class="p">,</span> <span class="s1">&#39;../Modules/&#39;</span><span class="p">))</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">layer_conductance</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">orig_sys_path</span>


<div class="viewcode-block" id="Encoder">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.Encoder">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple feed-forward neural network used as an encoder with two linear layers separated by dropout and batch normalization.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        encoder (nn.ModuleList): Contains the linear layers of the encoder.</span>
<span class="sd">        norm (nn.ModuleList): Contains the batch normalization layers corresponding to each encoder layer.</span>
<span class="sd">        decoder (torch.nn.Sequential): A sequential module containing the decoder part of the model.</span>
<span class="sd">        drop (nn.Dropout): Dropout layer to prevent overfitting.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dim (int): Dimensionality of the input features.</span>
<span class="sd">        latent_dim (int): Dimensionality of the latent space (middle layer&#39;s output).</span>
<span class="sd">        output_dim (int): Dimensionality of the output features after decoding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">input_dim</span> <span class="p">,</span> <span class="n">latent_dim</span> <span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span> <span class="p">,</span> <span class="mi">500</span><span class="p">),</span> 
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">),</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> 

<div class="viewcode-block" id="Encoder.forward">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.Encoder.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="p">:</span> 
            <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">layer</span><span class="p">](</span><span class="n">encoded</span><span class="p">)</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="n">layer</span><span class="p">](</span><span class="n">encoded</span><span class="p">)</span>
            
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">decoded</span> </div>
</div>

    
<div class="viewcode-block" id="GSage_MME">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GSage_MME">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GSage_MME</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A multi-modal GraphSAGE model utilizing encoder modules for initial feature transformation, applying GraphSAGE convolution over the graph structure.</span>

<span class="sd">    This model combines several data modalities, each processed by separate encoders, integrates the encoded features, and performs graph-based learning to produce node embeddings or class scores.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        encoder_dims (nn.ModuleList): List of encoder modules for each modality of data.</span>
<span class="sd">        gnnlayers (nn.ModuleList): List of GraphSAGE convolution layers for propagating and transforming node features across the graph.</span>
<span class="sd">        batch_norms (nn.ModuleList): Batch normalization applied to the outputs of GNN layers except the last layer.</span>
<span class="sd">        num_layers (int): Total number of GNN layers.</span>
<span class="sd">        input_dims (list): List of input dimensions, one for each data modality.</span>
<span class="sd">        hidden_feats (list): List of the feature dimensions for each hidden layer in the GNN.</span>
<span class="sd">        num_classes (int): Number of output classes or the dimension of the output features.</span>
<span class="sd">        drop (nn.Dropout): Dropout layer applied after each GNN layer for regularization.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dims (list): Input dimensions for each modality of input data.</span>
<span class="sd">        latent_dims (list): Latent dimensions for corresponding encoders processing each modality of input data.</span>
<span class="sd">        decoder_dim (int): Unified dimension to which all modalities are decoded.</span>
<span class="sd">        hidden_feats (list): Dimensions for hidden layers of the GNN.</span>
<span class="sd">        num_classes (int): Number of classes for classification tasks.</span>
<span class="sd">        PNet (optional): A PNet model for embedding pathway networks, used as an optional modality-specific encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,</span> <span class="n">latent_dims</span> <span class="p">,</span> <span class="n">decoder_dim</span> <span class="p">,</span> <span class="n">hidden_feats</span> <span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">PNet</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span> <span class="o">=</span> <span class="n">input_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span> <span class="o">=</span> <span class="n">hidden_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="c1"># GCN with Encoder reduced dim input and pooling scheme</span>
        <span class="k">for</span> <span class="n">modality</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_dims</span><span class="p">)):</span>  <span class="c1"># excluding the input layer</span>
            <span class="k">if</span> <span class="n">PNet</span> <span class="o">!=</span> <span class="kc">None</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PNET</span><span class="p">(</span><span class="n">reactome_network</span><span class="o">=</span><span class="n">PNet</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">decoder_dim</span><span class="p">,</span> 
                      <span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">,</span> <span class="n">filter_pathways</span><span class="o">=</span><span class="kc">False</span> <span class="p">,</span> <span class="n">input_layer_mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">))</span>
            <span class="k">else</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">input_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">latent_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">decoder_dim</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">layers</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span> <span class="p">:</span>
            <span class="k">if</span> <span class="n">layers</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span><span class="mi">1</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">layers</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">SAGEConv</span><span class="p">(</span><span class="n">decoder_dim</span> <span class="p">,</span> <span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">]</span> <span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span> <span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">SAGEConv</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">]</span> <span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">]))</span>
            <span class="k">else</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">SAGEConv</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span> <span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span>
                <span class="p">)</span>
                
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<div class="viewcode-block" id="GSage_MME.forward">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GSage_MME.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass for GSage_MME embedding computation.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): Input graph.</span>
<span class="sd">            h (torch.Tensor): Feature matrix.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Output after passing through the GNN layers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>
            
            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">l</span> <span class="p">,</span> <span class="p">(</span><span class="n">layer</span> <span class="p">,</span> <span class="n">g_layer</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span> <span class="p">,</span> <span class="n">g</span><span class="p">))</span> <span class="p">:</span> 
            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">g_layer</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">h</span></div>

    
<div class="viewcode-block" id="GSage_MME.inference">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GSage_MME.inference">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">device</span> <span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform a forward pass using the model for inference, without computing gradients. Usually used after the model has been trained.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): The DGL graph on which inference is performed.</span>
<span class="sd">            h (torch.Tensor): Node features for all nodes in the graph.</span>
<span class="sd">            device (torch.device): The device tensors will be sent to.</span>
<span class="sd">            batch_size (int): The size of batches to use during inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The outputs of the inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MultiLayerFullNeighborSampler</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_node_feats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">])</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">g</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="n">sampler</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">buffer_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">buffer_device</span> <span class="o">!=</span> <span class="n">device</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">buffer_device</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># len(blocks) = 1</span>
                <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                <span class="c1"># by design, our output nodes are contiguous</span>
                <span class="n">y</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">output_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">buffer_device</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span></div>

    
<div class="viewcode-block" id="GSage_MME.embedding_extraction">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GSage_MME.embedding_extraction">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">embedding_extraction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">device</span> <span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract embeddings for the nodes in the graph. This method is typically used to retrieve node embeddings that can then be used for visualization, clustering, or as input for downstream tasks.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): The graph for which embeddings are to be retrieved.</span>
<span class="sd">            h (torch.Tensor): Node features tensor.</span>
<span class="sd">            device (torch.device): The device to perform computations on.</span>
<span class="sd">            batch_size (int): Size of the batches to use during the computation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Node embeddings extracted by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MultiLayerFullNeighborSampler</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_node_feats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">])</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">g</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="n">sampler</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">buffer_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">buffer_device</span> <span class="o">!=</span> <span class="n">device</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">buffer_device</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># len(blocks) = 1</span>
                <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                <span class="c1"># by design, our output nodes are contiguous</span>
                <span class="n">y</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">output_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">buffer_device</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span></div>
</div>

    
<div class="viewcode-block" id="GCN_MME">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GCN_MME">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GCN_MME</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A multi-modal GraphSAGE model utilizing encoder modules for initial feature transformation, applying GraphConv convolution over the graph structure.</span>

<span class="sd">    This model combines several data modalities, each processed by separate encoders, integrates the encoded features, and performs graph-based learning to produce node embeddings or class scores.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        encoder_dims (nn.ModuleList): List of encoder modules for each modality of data.</span>
<span class="sd">        gnnlayers (nn.ModuleList): List of GraphConv convolution layers for propagating and transforming node features across the graph.</span>
<span class="sd">        batch_norms (nn.ModuleList): Batch normalization applied to the outputs of GNN layers except the last layer.</span>
<span class="sd">        num_layers (int): Total number of GNN layers.</span>
<span class="sd">        input_dims (list): List of input dimensions, one for each data modality.</span>
<span class="sd">        hidden_feats (list): List of the feature dimensions for each hidden layer in the GNN.</span>
<span class="sd">        num_classes (int): Number of output classes or the dimension of the output features.</span>
<span class="sd">        drop (nn.Dropout): Dropout layer applied after each GNN layer for regularization.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dims (list): Input dimensions for each modality of input data.</span>
<span class="sd">        latent_dims (list): Latent dimensions for corresponding encoders processing each modality of input data.</span>
<span class="sd">        decoder_dim (int): Unified dimension to which all modalities are decoded.</span>
<span class="sd">        hidden_feats (list): Dimensions for hidden layers of the GNN.</span>
<span class="sd">        num_classes (int): Number of classes for classification tasks.</span>
<span class="sd">        PNet (optional): A PNet model for embedding pathway networks, used as an optional modality-specific encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,</span> <span class="n">latent_dims</span> <span class="p">,</span> <span class="n">decoder_dim</span> <span class="p">,</span> <span class="n">hidden_feats</span> <span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">PNet</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span> <span class="o">=</span> <span class="n">input_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span> <span class="o">=</span> <span class="n">hidden_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="c1"># GCN with Encoder reduced dim input and pooling scheme</span>
        
        <span class="k">for</span> <span class="n">modality</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_dims</span><span class="p">)):</span>  <span class="c1"># excluding the input layer</span>
            <span class="k">if</span> <span class="n">PNet</span> <span class="o">!=</span> <span class="kc">None</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PNET</span><span class="p">(</span><span class="n">reactome_network</span><span class="o">=</span><span class="n">PNet</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">decoder_dim</span><span class="p">,</span> 
                      <span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">,</span> <span class="n">filter_pathways</span><span class="o">=</span><span class="kc">True</span> <span class="p">,</span> <span class="n">input_layer_mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">))</span>
            <span class="k">else</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">input_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">latent_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">decoder_dim</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">layers</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span> <span class="p">:</span>
            <span class="k">if</span> <span class="n">layers</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span><span class="mi">1</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">layers</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">GraphConv</span><span class="p">(</span><span class="n">decoder_dim</span> <span class="p">,</span> <span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span>
                    <span class="p">)</span>
                <span class="k">else</span> <span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">GraphConv</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">]))</span>
            <span class="k">else</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">GraphConv</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">)</span>
                <span class="p">)</span>
                
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<div class="viewcode-block" id="GCN_MME.forward">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GCN_MME.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">g</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass for GSage_MME embedding computation.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): Input graph.</span>
<span class="sd">            h (torch.Tensor): Feature matrix.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Output after passing through the GNN layers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">n</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">][</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">]))</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>
            
            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">l</span> <span class="p">,</span> <span class="p">(</span><span class="n">layer</span> <span class="p">,</span> <span class="n">g_layer</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span> <span class="p">,</span> <span class="n">g</span><span class="p">))</span> <span class="p">:</span>             
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">g_layer</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">x</span></div>

    
<div class="viewcode-block" id="GCN_MME.inference">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GCN_MME.inference">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">device</span> <span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform a forward pass using the model for inference, without computing gradients. Usually used after the model has been trained.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): The DGL graph on which inference is performed.</span>
<span class="sd">            h (torch.Tensor): Node features for all nodes in the graph.</span>
<span class="sd">            device (torch.device): The device tensors will be sent to.</span>
<span class="sd">            batch_size (int): The size of batches to use during inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The outputs of the inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MultiLayerFullNeighborSampler</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_node_feats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">])</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">g</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="n">sampler</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">buffer_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">buffer_device</span> <span class="o">!=</span> <span class="n">device</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">buffer_device</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># len(blocks) = 1</span>
                <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                <span class="c1"># by design, our output nodes are contiguous</span>
                <span class="n">y</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">output_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">buffer_device</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span></div>

    
<div class="viewcode-block" id="GCN_MME.embedding_extraction">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GCN_MME.embedding_extraction">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">embedding_extraction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">device</span> <span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract embeddings for the nodes in the graph. This method is typically used to retrieve node embeddings that can then be used for visualization, clustering, or as input for downstream tasks.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): The graph for which embeddings are to be retrieved.</span>
<span class="sd">            h (torch.Tensor): Node features tensor.</span>
<span class="sd">            device (torch.device): The device to perform computations on.</span>
<span class="sd">            batch_size (int): Size of the batches to use during the computation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Node embeddings extracted by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MultiLayerFullNeighborSampler</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_node_feats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">])</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">g</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="n">sampler</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">buffer_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">buffer_device</span> <span class="o">!=</span> <span class="n">device</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">buffer_device</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># len(blocks) = 1</span>
                <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                <span class="c1"># by design, our output nodes are contiguous</span>
                <span class="n">y</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">output_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">buffer_device</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="GCN_MME.feature_importance">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GCN_MME.feature_importance">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">feature_importance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_dataloader</span> <span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate feature importances using the Conductance algorithm through Captum.</span>

<span class="sd">        Args:</span>
<span class="sd">            test_dataset (torch.Tensor): The dataset for which to calculate importances.</span>
<span class="sd">            target_class (int): The target class index for which to calculate importances.</span>

<span class="sd">        Returns:</span>
<span class="sd">            pd.DataFrame: A dataframe containing the feature importances.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Feature Level Importance&#39;</span><span class="p">)</span>
        <span class="n">feature_importances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">max</span><span class="p">(</span><span class="n">test_dataloader</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">pnet</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 
            <span class="n">cond</span> <span class="o">=</span> <span class="n">layer_conductance</span><span class="o">.</span><span class="n">LayerConductance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pnet</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">):</span>
                <span class="n">feature_importances_batch</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="p">[</span><span class="n">output_nodes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                
                <span class="n">x</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">srcdata</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">]</span>
                <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">blocks</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                
                <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">prev_dim</span> <span class="o">+</span> <span class="n">dim</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    
                <span class="k">for</span> <span class="n">target_class</span> <span class="ow">in</span> <span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span> <span class="p">:</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="p">:</span> 
                        <span class="n">conductance</span> <span class="o">=</span> <span class="n">cond</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target_class</span><span class="p">,</span> <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> <span class="n">internal_batch_size</span> <span class="o">=</span><span class="mi">128</span> <span class="p">,</span> <span class="n">attribute_to_layer_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
                        
                    <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                        <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                        <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
                    <span class="n">cond_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">conductance</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">conductance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)])[</span><span class="n">reindex</span><span class="p">]</span>
                        
                    <span class="n">cond_output</span> <span class="o">=</span> <span class="n">cond_imputed</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">input_nodes</span> <span class="p">,</span> <span class="n">output_nodes</span><span class="p">)]</span>

                    <span class="n">feature_importances_batch</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">prev_dim</span> <span class="o">+</span> <span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">cond_output</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">]</span>
                    <span class="k">del</span> <span class="n">conductance</span>
                    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

                <span class="n">feature_importances</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_importances_batch</span>
                    
            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="k">del</span> <span class="n">cond</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pnet</span><span class="p">,</span> <span class="s1">&#39;data_index&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="o">.</span><span class="n">indices</span><span class="p">)))</span>

        <span class="n">feature_importances</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="p">[</span><span class="n">test_dataloader</span><span class="o">.</span><span class="n">indices</span><span class="p">]</span>

        <span class="n">feature_importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">feature_importances</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                                           <span class="n">index</span><span class="o">=</span><span class="n">data_index</span><span class="p">,</span>
                                           <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_importances</span> <span class="o">=</span> <span class="n">feature_importances</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_importances</span></div>


<div class="viewcode-block" id="GCN_MME.layerwise_importance">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GCN_MME.layerwise_importance">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">layerwise_importance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_dataloader</span> <span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute layer-wise importance scores across all layers for given targets.</span>

<span class="sd">        Args:</span>
<span class="sd">            test_dataset (torch.Tensor): The dataset for which to calculate importances.</span>
<span class="sd">            target_class (int): The target class index for importance calculation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[pd.DataFrame]: A list containing the importance scores for each layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">layer_importance_scores</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">pnet</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 
            <span class="n">layer_importance_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">lvl</span> <span class="p">,</span> <span class="n">level</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pnet</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">level</span><span class="p">)</span>
                <span class="n">cond</span> <span class="o">=</span> <span class="n">layer_conductance</span><span class="o">.</span><span class="n">LayerConductance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>

                <span class="n">cond_vals</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">):</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">srcdata</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">]</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">blocks</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                    
                    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">prev_dim</span> <span class="o">+</span> <span class="n">dim</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

                    <span class="n">cond_vals_tmp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">level</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                        
                    <span class="k">for</span> <span class="n">target_class</span> <span class="ow">in</span> <span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span> <span class="p">:</span>
                        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="p">:</span> 
                            <span class="n">conductance</span> <span class="o">=</span> <span class="n">cond</span><span class="o">.</span><span class="n">attribute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target_class</span><span class="p">,</span> <span class="n">additional_forward_args</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> <span class="n">internal_batch_size</span> <span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

                        <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
                        <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                            <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                            <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
                        <span class="n">cond_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">conductance</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">conductance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)])[</span><span class="n">reindex</span><span class="p">]</span>
                            
                        <span class="n">cond_output</span> <span class="o">=</span> <span class="n">cond_imputed</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">input_nodes</span> <span class="p">,</span> <span class="n">output_nodes</span><span class="p">)]</span>

                        <span class="n">cond_vals_tmp</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">]</span> <span class="o">=</span> <span class="n">cond_output</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">]</span>

                        <span class="k">del</span> <span class="n">conductance</span>
                        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
                    
                    <span class="n">cond_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_vals_tmp</span><span class="p">)</span>

                <span class="n">cond_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">cond_vals</span> <span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                
                <span class="n">cols</span> <span class="o">=</span> <span class="n">pnet</span><span class="o">.</span><span class="n">layer_info</span><span class="p">[</span><span class="n">lvl</span><span class="p">]</span>
                <span class="n">data_index</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pnet</span><span class="p">,</span> <span class="s1">&#39;data_index&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="o">.</span><span class="n">indices</span><span class="p">)))</span>
        
                <span class="n">cond_vals_genomic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cond_vals</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                                                 <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span>
                                                 <span class="n">index</span><span class="o">=</span><span class="n">data_index</span><span class="p">)</span>
                <span class="n">layer_importance_scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cond_vals_genomic</span><span class="p">)</span>


                <span class="k">del</span> <span class="n">cond</span>
                <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>
        
        <span class="k">return</span> <span class="n">layer_importance_scores</span></div>
</div>

    
<div class="viewcode-block" id="GAT_MME">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GAT_MME">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GAT_MME</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A multi-modal GAT (Graph Attention Network) model utilizing encoder modules for initial feature transformation, applying GATConv  convolution over the graph structure.</span>

<span class="sd">    This model combines several data modalities, each processed by separate encoders, integrates the encoded features, and performs graph-based learning to produce node embeddings or class scores.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        encoder_dims (nn.ModuleList): List of encoder modules for each modality of data.</span>
<span class="sd">        gnnlayers (nn.ModuleList): List of GATConv convolution layers for propagating and transforming node features across the graph.</span>
<span class="sd">        batch_norms (nn.ModuleList): Batch normalization applied to the outputs of GNN layers except the last layer.</span>
<span class="sd">        num_layers (int): Total number of GNN layers.</span>
<span class="sd">        input_dims (list): List of input dimensions, one for each data modality.</span>
<span class="sd">        hidden_feats (list): List of the feature dimensions for each hidden layer in the GNN.</span>
<span class="sd">        num_classes (int): Number of output classes or the dimension of the output features.</span>
<span class="sd">        drop (nn.Dropout): Dropout layer applied after each GNN layer for regularization.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dims (list): Input dimensions for each modality of input data.</span>
<span class="sd">        latent_dims (list): Latent dimensions for corresponding encoders processing each modality of input data.</span>
<span class="sd">        decoder_dim (int): Unified dimension to which all modalities are decoded.</span>
<span class="sd">        hidden_feats (list): Dimensions for hidden layers of the GNN.</span>
<span class="sd">        num_classes (int): Number of classes for classification tasks.</span>
<span class="sd">        PNet (optional): A PNet model for embedding pathway networks, used as an optional modality-specific encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,</span> <span class="n">latent_dims</span> <span class="p">,</span> <span class="n">decoder_dim</span> <span class="p">,</span> <span class="n">hidden_feats</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">PNet</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span>   <span class="o">=</span> <span class="n">input_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span> <span class="o">=</span> <span class="n">hidden_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>        <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span>  <span class="o">=</span> <span class="n">num_classes</span>

        <span class="c1"># GCN with Encoder reduced dim input and pooling scheme</span>
        <span class="k">for</span> <span class="n">modality</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_dims</span><span class="p">)):</span>  <span class="c1"># excluding the input layer</span>
            <span class="k">if</span> <span class="n">PNet</span> <span class="o">!=</span> <span class="kc">None</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PNET</span><span class="p">(</span><span class="n">reactome_network</span><span class="o">=</span><span class="n">PNet</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">decoder_dim</span><span class="p">,</span> 
                      <span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">,</span> <span class="n">filter_pathways</span><span class="o">=</span><span class="kc">False</span> <span class="p">,</span> <span class="n">input_layer_mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">))</span>
            <span class="k">else</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">input_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">latent_dims</span><span class="p">[</span><span class="n">modality</span><span class="p">]</span> <span class="p">,</span> <span class="n">decoder_dim</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">layers</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span> <span class="p">:</span>
            <span class="k">if</span> <span class="n">layers</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span><span class="mi">1</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">layers</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">GATConv</span><span class="p">(</span><span class="n">decoder_dim</span> <span class="p">,</span> <span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">],</span> <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span>
                    <span class="p">)</span>
                <span class="k">else</span> <span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">GATConv</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">],</span> <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">layers</span><span class="p">]))</span>
            <span class="k">else</span> <span class="p">:</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">GATConv</span><span class="p">(</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">layers</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span>
                <span class="p">)</span>
                
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<div class="viewcode-block" id="GAT_MME.forward">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GAT_MME.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass for GSage_MME embedding computation.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): Input graph.</span>
<span class="sd">            h (torch.Tensor): Feature matrix.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Output after passing through the GNN layers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>
            
            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">l</span> <span class="p">,</span> <span class="p">(</span><span class="n">layer</span> <span class="p">,</span> <span class="n">g_layer</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span> <span class="p">,</span> <span class="n">g</span><span class="p">))</span> <span class="p">:</span> 
            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">g_layer</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># last layer</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># other layer(s)</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">h</span></div>

    
<div class="viewcode-block" id="GAT_MME.inference">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GAT_MME.inference">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">device</span> <span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform a forward pass using the model for inference, without computing gradients. Usually used after the model has been trained.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): The DGL graph on which inference is performed.</span>
<span class="sd">            h (torch.Tensor): Node features for all nodes in the graph.</span>
<span class="sd">            device (torch.device): The device tensors will be sent to.</span>
<span class="sd">            batch_size (int): The size of batches to use during inference.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The outputs of the inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MultiLayerFullNeighborSampler</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_node_feats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">])</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">g</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="n">sampler</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">buffer_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">buffer_device</span> <span class="o">!=</span> <span class="n">device</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">buffer_device</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># len(blocks) = 1</span>
                <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># last layer</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other layer(s)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                <span class="c1"># by design, our output nodes are contiguous</span>
                <span class="n">y</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">output_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">buffer_device</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span></div>

    
<div class="viewcode-block" id="GAT_MME.embedding_extraction">
<a class="viewcode-back" href="../MOGDx/MAIN/GNN_MME.html#GNN_MME.GAT_MME.embedding_extraction">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">embedding_extraction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span> <span class="p">,</span> <span class="n">h</span> <span class="p">,</span> <span class="n">device</span> <span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract embeddings for the nodes in the graph. This method is typically used to retrieve node embeddings that can then be used for visualization, clustering, or as input for downstream tasks.</span>

<span class="sd">        Args:</span>
<span class="sd">            g (dgl.DGLGraph): The graph for which embeddings are to be retrieved.</span>
<span class="sd">            h (torch.Tensor): Node features tensor.</span>
<span class="sd">            device (torch.device): The device to perform computations on.</span>
<span class="sd">            batch_size (int): Size of the batches to use during the computation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Node embeddings extracted by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">prev_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="p">,</span> <span class="p">(</span><span class="n">Encoder</span> <span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_dims</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">))</span> <span class="p">:</span> 

            <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="p">,</span> <span class="n">prev_dim</span><span class="p">:</span><span class="n">dim</span><span class="o">+</span><span class="n">prev_dim</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">nan_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">nan_rows</span><span class="p">])</span>

            <span class="n">imputed_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_rows</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reindex</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">imp_idx</span> <span class="ow">in</span> <span class="n">imputed_idx</span> <span class="p">:</span>
                <span class="n">reindex</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">imp_idx</span><span class="p">,</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Insert the last index at the desired position</span>
                <span class="k">del</span> <span class="n">reindex</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">decoded_imputed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">decoded</span> <span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">decoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="p">,</span> <span class="n">decoded</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])[</span><span class="n">reindex</span><span class="p">]</span>

            <span class="n">node_features</span> <span class="o">+=</span> <span class="n">decoded_imputed</span>

            <span class="n">prev_dim</span> <span class="o">+=</span> <span class="n">dim</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sampler</span> <span class="o">=</span> <span class="n">MultiLayerFullNeighborSampler</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefetch_node_feats</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;feat&quot;</span><span class="p">])</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">g</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="n">sampler</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">buffer_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">buffer_device</span> <span class="o">!=</span> <span class="n">device</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_feats</span><span class="p">[</span><span class="n">l</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">buffer_device</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># len(blocks) = 1</span>
                <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnnlayers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1">#second last layer</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other layer(s)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norms</span><span class="p">[</span><span class="n">l</span><span class="p">](</span><span class="n">h</span><span class="p">)</span>
                <span class="c1"># by design, our output nodes are contiguous</span>
                <span class="n">y</span><span class="p">[</span><span class="n">output_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="n">output_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">buffer_device</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z09VV2YFE0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-Z09VV2YFE0', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>